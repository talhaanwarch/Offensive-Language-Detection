{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OLIC_Classification_GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talhaanwarch/Offensive-Language-Detection/blob/master/OLID_Transform%20Network%20LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GhlQnoqqBy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pkvGyUqqCMG",
        "colab_type": "code",
        "outputId": "54f92fa0-c847-4395-97f2-d59861ff4d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nifo-yegqBrd",
        "colab_type": "code",
        "outputId": "e4ab2ac3-0b32-4cdc-88d5-9259707ebd35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "cd /content/drive/My Drive/dataset/OLID"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/dataset/OLID\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIZVWJBiqPth",
        "colab_type": "code",
        "outputId": "bde675a1-1726-4d8c-ca17-0e422118a462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elmo-model.h5      labels-levelb.csv       testset-levelb.tsv\n",
            "glove.6B.100d.txt  labels-levelc.csv       testset-levelc.tsv\n",
            "glove.6B.200d.txt  olid-training-v1.0.tsv\n",
            "labels-levela.csv  testset-levela.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnOwTGAXpLvI",
        "colab_type": "code",
        "outputId": "5ac097c4-c655-4faa-fd05-6948e9692cf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpYZxDnjpo32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv( 'olid-training-v1.0.tsv',sep=\"\\t\")\n",
        "test=pd.read_csv('testset-levela.tsv',sep=\"\\t\")\n",
        "y_test=pd.read_csv( 'labels-levela.csv',header=None).iloc[:,-1]\n",
        "#OFF=0 \n",
        "#NOT=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reTRftvWpUJk",
        "colab_type": "code",
        "outputId": "afd4e07e-2af2-4ef4-c631-50b2460684fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "y_train=train['subtask_a']\n",
        "train=train['tweet']\n",
        "test=test['tweet']\n",
        "y_train=pd.factorize(y_train)[0]\n",
        "y_test=pd.factorize(y_test)[0]\n",
        "\n",
        "import collections\n",
        "collections.Counter(y_train)\n",
        "\n",
        "#Counter({0: 4400, 1: 8840})"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 4400, 1: 8840})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KkTgzF_q4s1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the training corpus\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "corpus_train = []\n",
        "for i in train:\n",
        "    x=i.lower()\n",
        "    x=x.replace('@user','')\n",
        "    x=x.replace('@[\\w\\-]+','')\n",
        "    #x=x.translate(str.maketrans('', '', string.punctuation))\n",
        "    x = re.sub('[^A-Za-z]', ' ', x)\n",
        "    #x=re.sub('\\s+',' ',x)\n",
        "    x=re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',x) #url\n",
        "    x = [lemmatizer.lemmatize(token) for token in x.split(\" \")]\n",
        "    x = [word for word in x if not word in stop_words]\n",
        "    x=\" \".join(x)\n",
        "    corpus_train.append(x)    \n",
        "# Creating the training corpus\n",
        "corpus_test = []\n",
        "for i in test:\n",
        "    x=i.lower()\n",
        "    x=x.replace('@user','')\n",
        "    x=x.replace('@[\\w\\-]+','')\n",
        "    #x=x.translate(str.maketrans('', '', string.punctuation))\n",
        "    x = re.sub('[^A-Za-z]', ' ', x)\n",
        "    #x=re.sub('\\s+',' ',x)\n",
        "    x=re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',x) #url\n",
        "    x = [lemmatizer.lemmatize(token) for token in x.split(\" \")]\n",
        "    x = [word for word in x if not word in stop_words]\n",
        "    x=\" \".join(x)\n",
        "    corpus_test.append(x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyyHvozBq7kf",
        "colab_type": "code",
        "outputId": "94ae585a-e00e-4af6-efeb-f6f07e1f1e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "max_words = 1000 #frequency of words to be kept\n",
        "max_len = 200\n",
        "\n",
        "tokenize = Tokenizer(num_words=max_words)\n",
        "tokenize.fit_on_texts(corpus_train)\n",
        "sequences = tokenize.texts_to_sequences(corpus_train)\n",
        "word_index = tokenize.word_index\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyxVJQ45rSW3",
        "colab_type": "code",
        "outputId": "241c689f-02ca-4216-bc5a-0f97fce5875d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import os\n",
        "embeddings_index = {}\n",
        "f = open('glove.6B.200d.txt',encoding='utf8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dwdxvp1rV6N",
        "colab_type": "code",
        "outputId": "ac0b04e0-05d9-47e2-96e8-ab9c488f73fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "num_words = min(max_words, len(word_index)) + 1\n",
        "print(num_words)\n",
        "\n",
        "embedding_dim = 200\n",
        "\n",
        "# first create a matrix of zeros, this is our embedding matrix\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "# for each word in out tokenizer lets try to find that work in our w2v model\n",
        "for word, i in word_index.items():\n",
        "    if i > max_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # we found the word - add that words vector to the matrix\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        # doesn't exist, assign a random vector\n",
        "        embedding_matrix[i] = np.random.randn(embedding_dim)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0Kh8XAn2z4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size=embedding_dim\n",
        "X=embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx6u4LLB3PDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "96b086b4-99c0-4d71-fe79-2fadb875bd81"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1001, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT0VJn4o8JHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.kaggle.com/buchan/transformer-network-with-1d-cnn-feature-extraction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ti71vuBrvhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random, os, sys\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "from keras.initializers import *\n",
        "import tensorflow as tf\n",
        "from keras.engine.topology import Layer\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "class LayerNormalization(Layer):\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "        self.eps = eps\n",
        "        super(LayerNormalization, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
        "                                     initializer=Ones(), trainable=True)\n",
        "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
        "                                    initializer=Zeros(), trainable=True)\n",
        "        super(LayerNormalization, self).build(input_shape)\n",
        "    def call(self, x):\n",
        "        mean = K.mean(x, axis=-1, keepdims=True)\n",
        "        std = K.std(x, axis=-1, keepdims=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "class ScaledDotProductAttention():\n",
        "    def __init__(self, d_model, attn_dropout=0.1):\n",
        "        self.temper = np.sqrt(d_model)\n",
        "        self.dropout = Dropout(attn_dropout)\n",
        "    def __call__(self, q, k, v, mask):\n",
        "        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\n",
        "        if mask is not None:\n",
        "            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n",
        "            attn = Add()([attn, mmask])\n",
        "        attn = Activation('softmax')(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n",
        "        return output, attn\n",
        "\n",
        "class MultiHeadAttention():\n",
        "    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n",
        "        self.mode = mode\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "        self.dropout = dropout\n",
        "        if mode == 0:\n",
        "            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n",
        "            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n",
        "            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n",
        "        elif mode == 1:\n",
        "            self.qs_layers = []\n",
        "            self.ks_layers = []\n",
        "            self.vs_layers = []\n",
        "            for _ in range(n_head):\n",
        "                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
        "                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n",
        "                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n",
        "        self.attention = ScaledDotProductAttention(d_model)\n",
        "        self.layer_norm = LayerNormalization() if use_norm else None\n",
        "        self.w_o = TimeDistributed(Dense(d_model))\n",
        "\n",
        "    def __call__(self, q, k, v, mask=None):\n",
        "        d_k, d_v = self.d_k, self.d_v\n",
        "        n_head = self.n_head\n",
        "\n",
        "        if self.mode == 0:\n",
        "            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n",
        "            ks = self.ks_layer(k)\n",
        "            vs = self.vs_layer(v)\n",
        "\n",
        "            def reshape1(x):\n",
        "                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n",
        "                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n",
        "                x = tf.transpose(x, [2, 0, 1, 3])  \n",
        "                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n",
        "                return x\n",
        "            qs = Lambda(reshape1)(qs)\n",
        "            ks = Lambda(reshape1)(ks)\n",
        "            vs = Lambda(reshape1)(vs)\n",
        "\n",
        "            if mask is not None:\n",
        "                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n",
        "            head, attn = self.attention(qs, ks, vs, mask=mask)  \n",
        "                \n",
        "            def reshape2(x):\n",
        "                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n",
        "                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n",
        "                x = tf.transpose(x, [1, 2, 0, 3])\n",
        "                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n",
        "                return x\n",
        "            head = Lambda(reshape2)(head)\n",
        "        elif self.mode == 1:\n",
        "            heads = []; attns = []\n",
        "            for i in range(n_head):\n",
        "                qs = self.qs_layers[i](q)   \n",
        "                ks = self.ks_layers[i](k) \n",
        "                vs = self.vs_layers[i](v) \n",
        "                head, attn = self.attention(qs, ks, vs, mask)\n",
        "                heads.append(head); attns.append(attn)\n",
        "            head = Concatenate()(heads) if n_head > 1 else heads[0]\n",
        "            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n",
        "\n",
        "        outputs = self.w_o(head)\n",
        "        outputs = Dropout(self.dropout)(outputs)\n",
        "        if not self.layer_norm: return outputs, attn\n",
        "        # outputs = Add()([outputs, q]) # sl: fix\n",
        "        return self.layer_norm(outputs), attn\n",
        "\n",
        "class PositionwiseFeedForward():\n",
        "    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n",
        "        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n",
        "        self.w_2 = Conv1D(d_hid, 1)\n",
        "        self.layer_norm = LayerNormalization()\n",
        "        self.dropout = Dropout(dropout)\n",
        "    def __call__(self, x):\n",
        "        output = self.w_1(x) \n",
        "        output = self.w_2(output)\n",
        "        output = self.dropout(output)\n",
        "        output = Add()([output, x])\n",
        "        return self.layer_norm(output)\n",
        "\n",
        "class EncoderLayer():\n",
        "    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n",
        "        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n",
        "    def __call__(self, enc_input, mask=None):\n",
        "        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n",
        "        output = self.pos_ffn_layer(output)\n",
        "        return output, slf_attn\n",
        "\n",
        "\n",
        "def GetPosEncodingMatrix(max_len, d_emb):\n",
        "    pos_enc = np.array([\n",
        "        [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] \n",
        "        if pos != 0 else np.zeros(d_emb) \n",
        "            for pos in range(max_len)\n",
        "            ])\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n",
        "    return pos_enc\n",
        "\n",
        "def GetPadMask(q, k):\n",
        "    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n",
        "    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n",
        "    mask = K.batch_dot(ones, mask, axes=[2,1])\n",
        "    return mask\n",
        "\n",
        "def GetSubMask(s):\n",
        "    len_s = tf.shape(s)[1]\n",
        "    bs = tf.shape(s)[:1]\n",
        "    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n",
        "    return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41kBO5_m0k_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "inp = Input(shape=(None, ))\n",
        "#x = Embedding(max_features, embed_size, weights=[embedding_matrix])(src_seq_input)\n",
        "x=Embedding(num_words,embedding_dim,embeddings_initializer=Constant(embedding_matrix),input_length=max_len,trainable=False)(inp)\n",
        "#x = Conv1D(32, kernel_size = 5, strides = 2, activation='relu')(x)\n",
        "#x = Conv1D(64, kernel_size = 5, strides = 2, activation='relu')(x)\n",
        "\n",
        "# LSTM before attention layers\n",
        "x = (CuDNNLSTM(64, return_sequences=True))(x) \n",
        "#x = (CuDNNLSTM(32, return_sequences=True))(x) \n",
        "\n",
        "x, slf_attn = MultiHeadAttention(n_head=3, d_model=200, d_k=64, d_v=64, dropout=0.1)(x, x, x)\n",
        "\n",
        "avg_pool = GlobalAveragePooling1D()(x)\n",
        "max_pool = GlobalMaxPooling1D()(x)\n",
        "conc = concatenate([avg_pool, max_pool])\n",
        "fc = Dense(512, activation=\"relu\")(conc)\n",
        "fc = Dense(512, activation=\"relu\")(fc)\n",
        "out = Dense(1, activation=\"sigmoid\")(fc)   \n",
        "\n",
        "\n",
        "model = Model(inputs=inp, outputs=out)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf6pnXqk47a7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "aba3db7a-2878-4f9a-9c4e-3824e00d93ad"
      },
      "source": [
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\n",
        "class_weights=dict(enumerate(class_weights))\n",
        "model.fit(sequences_matrix,y_train,batch_size=256,epochs=10,verbose=2,class_weight=class_weights,validation_split=0.2)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10592 samples, validate on 2648 samples\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.7195 - acc: 0.5585 - val_loss: 0.6480 - val_acc: 0.6133\n",
            "Epoch 2/10\n",
            " - 6s - loss: 0.6181 - acc: 0.6933 - val_loss: 0.5883 - val_acc: 0.7485\n",
            "Epoch 3/10\n",
            " - 6s - loss: 0.5744 - acc: 0.7232 - val_loss: 0.5887 - val_acc: 0.7526\n",
            "Epoch 4/10\n",
            " - 6s - loss: 0.5561 - acc: 0.7368 - val_loss: 0.5605 - val_acc: 0.7424\n",
            "Epoch 5/10\n",
            " - 6s - loss: 0.5429 - acc: 0.7463 - val_loss: 0.6093 - val_acc: 0.7659\n",
            "Epoch 6/10\n",
            " - 6s - loss: 0.5355 - acc: 0.7526 - val_loss: 0.5595 - val_acc: 0.7406\n",
            "Epoch 7/10\n",
            " - 6s - loss: 0.5338 - acc: 0.7576 - val_loss: 0.6143 - val_acc: 0.7353\n",
            "Epoch 8/10\n",
            " - 6s - loss: 0.5181 - acc: 0.7649 - val_loss: 0.5799 - val_acc: 0.7413\n",
            "Epoch 9/10\n",
            " - 6s - loss: 0.5107 - acc: 0.7720 - val_loss: 0.6201 - val_acc: 0.7372\n",
            "Epoch 10/10\n",
            " - 6s - loss: 0.5031 - acc: 0.7756 - val_loss: 0.6026 - val_acc: 0.7564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e1a2e1160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BD-ntr8rznW",
        "colab_type": "code",
        "outputId": "c9b78885-5cf1-4c94-abef-a7433b6ae39d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "test_sequences = tokenize.texts_to_sequences(corpus_test)\n",
        "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "print(model.evaluate(test_sequences_matrix,y_test))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 0s 256us/step\n",
            "[0.4500923339710679, 0.7930232552594917]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-fKEe-4rzp4",
        "colab_type": "code",
        "outputId": "44cd38a1-d5de-40d5-fe8f-464f033846f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(test_sequences_matrix, batch_size=128, verbose=1)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 0s 348us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.55      0.60       240\n",
            "           1       0.84      0.89      0.86       620\n",
            "\n",
            "    accuracy                           0.79       860\n",
            "   macro avg       0.74      0.72      0.73       860\n",
            "weighted avg       0.78      0.79      0.79       860\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72QjwwRRrzsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o9xT3kjyc7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}