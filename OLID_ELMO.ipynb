{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OLIC_Classification_GloVe_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talhaanwarch/Offensive-Language-Detection/blob/master/OLID_ELMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GhlQnoqqBy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pkvGyUqqCMG",
        "colab_type": "code",
        "outputId": "9438d108-6c45-418e-aafc-faa87b729244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nifo-yegqBrd",
        "colab_type": "code",
        "outputId": "c02a2bf5-fb14-4f0f-c13c-616a9f70c0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "cd /content/drive/My Drive/dataset/OLID"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/dataset/OLID\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIZVWJBiqPth",
        "colab_type": "code",
        "outputId": "653efceb-374a-4db0-b7eb-f55f9248feb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elmo-model.h5      labels-levelb.csv       testset-levelb.tsv\n",
            "glove.6B.100d.txt  labels-levelc.csv       testset-levelc.tsv\n",
            "glove.6B.200d.txt  olid-training-v1.0.tsv\n",
            "labels-levela.csv  testset-levela.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnOwTGAXpLvI",
        "colab_type": "code",
        "outputId": "dd0346ba-56d1-4b80-b05c-35abf327b882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpYZxDnjpo32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv( 'olid-training-v1.0.tsv',sep=\"\\t\")\n",
        "test=pd.read_csv('testset-levela.tsv',sep=\"\\t\")\n",
        "y_test=pd.read_csv( 'labels-levela.csv',header=None).iloc[:,-1]\n",
        "#OFF=0 \n",
        "#NOT=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reTRftvWpUJk",
        "colab_type": "code",
        "outputId": "ae908189-081d-482d-f289-6625a8409b41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "y_train=train['subtask_a']\n",
        "train=train['tweet']\n",
        "test=test['tweet']\n",
        "Y_train=pd.factorize(y_train)[0]\n",
        "Y_test=pd.factorize(y_test)[0]\n",
        "\n",
        "import collections\n",
        "collections.Counter(y_train)\n",
        "\n",
        "#Counter({0: 4400, 1: 8840})"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'NOT': 8840, 'OFF': 4400})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KkTgzF_q4s1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the training corpus\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "corpus_train = []\n",
        "for i in train:\n",
        "    x=i.lower()\n",
        "    x=x.replace('@user','')\n",
        "    x=x.replace('@[\\w\\-]+','')\n",
        "    #x=x.translate(str.maketrans('', '', string.punctuation))\n",
        "    x = re.sub('[^A-Za-z]', ' ', x)\n",
        "    #x=re.sub('\\s+',' ',x)\n",
        "    x=re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',x) #url\n",
        "    x = [lemmatizer.lemmatize(token) for token in x.split(\" \")]\n",
        "    x = [word for word in x if not word in stop_words]\n",
        "    x=\" \".join(x)\n",
        "    corpus_train.append(x)    \n",
        "# Creating the training corpus\n",
        "corpus_test = []\n",
        "for i in test:\n",
        "    x=i.lower()\n",
        "    x=x.replace('@user','')\n",
        "    x=x.replace('@[\\w\\-]+','')\n",
        "    #x=x.translate(str.maketrans('', '', string.punctuation))\n",
        "    x = re.sub('[^A-Za-z]', ' ', x)\n",
        "    #x=re.sub('\\s+',' ',x)\n",
        "    x=re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',x) #url\n",
        "    x = [lemmatizer.lemmatize(token) for token in x.split(\" \")]\n",
        "    x = [word for word in x if not word in stop_words]\n",
        "    x=\" \".join(x)\n",
        "    corpus_test.append(x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwy0XK9lkswX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "600250dd-7c2d-4dc4-f62c-7f4c1a565d1f"
      },
      "source": [
        "X_train=pd.DataFrame(corpus_train) \n",
        "X_train.columns=['tweet']\n",
        "X_train.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ask native american take</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>go home drunk      maga  trump          url</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>amazon investigating chinese employee selling ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>someone vetaken  piece shit volcano</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>obama wanted liberal  amp  illegals move red...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet\n",
              "0                          ask native american take \n",
              "1        go home drunk      maga  trump          url\n",
              "2  amazon investigating chinese employee selling ...\n",
              "3            someone vetaken  piece shit volcano    \n",
              "4    obama wanted liberal  amp  illegals move red..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k9TeM7xtEWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "ba2954c2-ef57-409e-d8ff-0db513973666"
      },
      "source": [
        "freq = pd.Series(' '.join(X_train['tweet']).split()).value_counts()[-10:]\n",
        "X_train['tweet']= X_train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "X_train['tweet'].head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                             ask native american take\n",
              "1                         go home drunk maga trump url\n",
              "2    amazon investigating chinese employee selling ...\n",
              "3                   someone vetaken piece shit volcano\n",
              "4     obama wanted liberal amp illegals move red state\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZDec_YEtRgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import time\n",
        "import pickle\n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyxVJQ45rSW3",
        "colab_type": "code",
        "outputId": "3839ca33-bae6-4442-cc7e-3e38edf19a4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "\n",
        "embed = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKnZoBZFuB_A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "1ac7ad35-e279-44fa-de40-87c38a0b0b41"
      },
      "source": [
        "from keras.layers import Input, Lambda, Dense\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "def ELMoEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n",
        "\n",
        "input_text = Input(shape=(1,), dtype=tf.string)\n",
        "embedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n",
        "dense1 = Dense(500, activation='relu')(embedding)\n",
        "dense2 = Dense(250, activation='relu')(dense1)\n",
        "pred = Dense(1, activation='sigmoid')(dense2)\n",
        "model = Model(inputs=[input_text], outputs=pred)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\n",
        "class_weights=dict(enumerate(class_weights))\n",
        "model.fit(X_train,Y_train,batch_size=128,epochs=10,verbose=2,class_weight=class_weights,validation_split=0.2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10592 samples, validate on 2648 samples\n",
            "Epoch 1/10\n",
            " - 35s - loss: 0.5796 - acc: 0.6885 - val_loss: 0.5163 - val_acc: 0.6979\n",
            "Epoch 2/10\n",
            " - 28s - loss: 0.5128 - acc: 0.7200 - val_loss: 0.5046 - val_acc: 0.7198\n",
            "Epoch 3/10\n",
            " - 29s - loss: 0.4872 - acc: 0.7426 - val_loss: 0.5340 - val_acc: 0.7126\n",
            "Epoch 4/10\n",
            " - 29s - loss: 0.4812 - acc: 0.7477 - val_loss: 0.4910 - val_acc: 0.7428\n",
            "Epoch 5/10\n",
            " - 29s - loss: 0.4631 - acc: 0.7594 - val_loss: 0.4995 - val_acc: 0.7394\n",
            "Epoch 6/10\n",
            " - 29s - loss: 0.4476 - acc: 0.7710 - val_loss: 0.5381 - val_acc: 0.7508\n",
            "Epoch 7/10\n",
            " - 28s - loss: 0.4475 - acc: 0.7758 - val_loss: 0.5080 - val_acc: 0.7572\n",
            "Epoch 8/10\n",
            " - 29s - loss: 0.4266 - acc: 0.7901 - val_loss: 0.5015 - val_acc: 0.7485\n",
            "Epoch 9/10\n",
            " - 28s - loss: 0.3965 - acc: 0.8050 - val_loss: 0.5468 - val_acc: 0.7474\n",
            "Epoch 10/10\n",
            " - 29s - loss: 0.3781 - acc: 0.8188 - val_loss: 0.5346 - val_acc: 0.7545\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fde71ab2be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BD-ntr8rznW",
        "colab_type": "code",
        "outputId": "d86a0f00-3ac7-4bb6-c8c7-89958cea81c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "X_test=pd.DataFrame(corpus_test) \n",
        "X_test.columns=['tweet']\n",
        "X_train.head()\n",
        "freq = pd.Series(' '.join(X_test['tweet']).split()).value_counts()[-10:]\n",
        "X_test['tweet']= X_train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "X_test['tweet'].head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                                                                                              ask native american take\n",
              "1                                                                                                                          go home drunk maga trump url\n",
              "2    amazon investigating chinese employee selling internal data third party seller looking edge competitive marketplace url amazon maga kag china tcot\n",
              "3                                                                                                                    someone vetaken piece shit volcano\n",
              "4                                                                                                      obama wanted liberal amp illegals move red state\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-fKEe-4rzp4",
        "colab_type": "code",
        "outputId": "9af0c3d3-aefd-43b6-8ed4-4d1c6ea27c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(X_test, batch_size=64, verbose=1)\n",
        "y_pred = np.round(y_pred)\n",
        "#y_pred = (y_pred > 0.5)\n",
        "\n",
        "print(classification_report(Y_test, y_pred))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 3s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.21      0.23       240\n",
            "           1       0.72      0.77      0.74       620\n",
            "\n",
            "    accuracy                           0.62       860\n",
            "   macro avg       0.49      0.49      0.49       860\n",
            "weighted avg       0.59      0.62      0.60       860\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72QjwwRRrzsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXj97Ba32JJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}